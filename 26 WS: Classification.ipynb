{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "REQUIREMENTS \nClassify sensor data in multiple categories: brushing teeth, climbing stairs, etc.\n\nREFERENCE \nhttps://spark.apache.org/docs/latest/ml-classification-regression.html \nhttps://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/classification.html\n\nThis notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Don't use it in production.\n\nIf running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, remove the Apache Spark setup in the first notebook cells."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 144kB/s  eta 0:00:01   |\u2588\u2588\u2588\u258f                            | 21.5MB 7.5MB/s eta 0:00:27| 23.9MB 7.5MB/s eta 0:00:26     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                        | 53.3MB 38.0MB/s eta 0:00:05     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                   | 83.3MB 36.5MB/s eta 0:00:04\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 202.1MB 40.1MB/s eta 0:00:01\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 38.9MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-10-02 06:37:33--  https://github.com/IBM/coursera/raw/master/coursera_ml/a2.parquet\nResolving github.com (github.com)... 140.82.113.3\nConnecting to github.com (github.com)|140.82.113.3|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/coursera_ml/a2.parquet [following]\n--2020-10-02 06:37:33--  https://github.com/IBM/skillsnetwork/raw/master/coursera_ml/a2.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ml/a2.parquet [following]\n--2020-10-02 06:37:34--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ml/a2.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 59032 (58K) [application/octet-stream]\nSaving to: \u2018a2.parquet\u2019\n\n100%[======================================>] 59,032      --.-K/s   in 0.003s  \n\n2020-10-02 06:37:34 (20.4 MB/s) - \u2018a2.parquet\u2019 saved [59032/59032]\n\n"
                }
            ],
            "source": "!wget https://github.com/IBM/coursera/raw/master/coursera_ml/a2.parquet"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it\u2019s time to have a look at the recorded sensor data. You should see data similar to the one exemplified below\u2026.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-----+-----------+-------------------+-------------------+-------------------+\n|CLASS|   SENSORID|                  X|                  Y|                  Z|\n+-----+-----------+-------------------+-------------------+-------------------+\n|    0|         26| 380.66434005495194| -139.3470983812975|-247.93697521077704|\n|    0|         29| 104.74324299209692| -32.27421440203938|-25.105013725863852|\n|    0| 8589934658| 118.11469236129976| 45.916682927433534| -87.97203782706572|\n|    0|34359738398| 246.55394030642543|-0.6122810693132044|-398.18662513951506|\n|    0|17179869241|-190.32584900181487|  234.7849657520335|-206.34483804019288|\n|    0|25769803830| 178.62396382387422| -47.07529438881511|  84.38310769821979|\n|    0|25769803831|  85.03128805189493|-4.3024316644854546|-1.1841857567516714|\n|    0|34359738411| 26.786262674736566| -46.33193951911338| 20.880756008396055|\n|    0| 8589934592|-16.203752396859194| 51.080957032176954| -96.80526656416971|\n|    0|25769803852|   47.2048142440404|  -78.2950899652916| 181.99604091494786|\n|    0|34359738369| 15.608872398939273| -79.90322809181754|  69.62150711098005|\n|    0|         19|-4.8281721129789315| -67.38050508399905| 221.24876396496404|\n|    0|         54| -98.40725712852762|-19.989364074314732|  -302.695196085276|\n|    0|17179869313| 22.835845394816594|   17.1633660118843| 32.877914832011385|\n|    0|34359738454|  84.20178070080324| -32.81572075916947| -48.63517643958031|\n|    0|          0|  56.54732521345129| -7.980106018032676|  95.05162719436447|\n|    0|17179869201|  -57.6008655247749|  5.135393798773895| 236.99158698947267|\n|    0|17179869308| -65.59264738389012| -48.92660057215126| -61.58970715383383|\n|    0|25769803790|  34.82337351291005|  9.483542084393937|  197.6066372962772|\n|    0|25769803825|  39.80573823439121|-0.7955236412785212| -79.66652640650325|\n+-----+-----------+-------------------+-------------------+-------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df=spark.read.load('a2.parquet')\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()\n\nsplits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Please create a VectorAssembler which consumes columns X, Y and Z and produces a column \u201cfeatures\u201d\n"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler(inputCols=[\"X\",\"Y\",\"Z\"], outputCol=\"features\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Please instantiate a classifier from the SparkML package and assign it to the classifier variable. Make sure to either\n1.\tRename the \u201cCLASS\u201d column to \u201clabel\u201d or\n2.\tSpecify the label-column correctly to be \u201cCLASS\u201d\n"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# REMEMBER the REQUIREMENTS: \n# Classify sensor data in multiple categories: brushing teeth, climbing stairs, etc. \n# Note: Since most of the Machine Learning algorithms work with numerical data, we map the target categories: \n# brushing teeth, climbing stairs, etc. to CLASS 0, 1, etc.\n# \n# Try solving the problem using the following Spark ML classifiers and compare the results (the prediction accuracy):\n# LogisticRegression \n# LinearSVC or Support Vector Machine (SVM)\n# DecisionTreeClassifier \n# RandomForestClassifier \n# GBTClassifier or Gradient-Boosted Trees \n# \n# REMEMBER: DECISION TREE vs RANDOM FOREST vs GRADIENT BOOSTING MACHINES \n# A decision tree is a simple, decision making-diagram. \n# Random forests are a large number of trees, combined (using averages or \"majority rules\") at the end of the process. \n# Gradient boosting machines also combine decision trees, but start the combining process at the beginning, instead of at the end.\n# \n# Reference links with code examples: \n# https://spark.apache.org/docs/latest/ml-classification-regression.html \n# https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/classification.html\n# \n# NOTE:\n# NaiveBayes CANNOT be used in this case since Naive Bayes requires nonnegative feature values\n# XGBoost is not included in Spark ML, but it is an implementation of Gradient Boosted Decision Trees designed for improved speed and performance.\n\n# SOULTION 1\nfrom pyspark.ml.classification import LogisticRegression\n# Make sure to also specify featuresCol and labelCol\n# classifier = ### YOUR CODE HERE ### \n\n# SOULTION 2: LinearSVC or Support Vector Machine (SVM)\n# from pyspark.ml.classification import LinearSVC \n# Make sure to also specify featuresCol and labelCol\n# classifier = ### YOUR CODE HERE ### \n\n# SOULTION 3\n# from pyspark.ml.classification import DecisionTreeClassifier\n# Make sure to also specify featuresCol and labelCol\n# classifier = ### YOUR CODE HERE ### \n\n# SOULTION 4\n# from pyspark.ml.classification import RandomForestClassifier\n# Make sure to also specify featuresCol and labelCol\n# classifier = ### YOUR CODE HERE ### \n\n# SOULTION 5\n# from pyspark.ml.classification import GBTClassifier\n# Make sure to also specify featuresCol and labelCol\n# classifier = ### YOUR CODE HERE ### \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let\u2019s train and evaluate\u2026\n"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, classifier])"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "model = pipeline.fit(df_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "prediction = model.transform(df_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-----+--------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+----------+\n|CLASS|SENSORID|                  X|                  Y|                  Z|            features|       rawPrediction|         probability|prediction|\n+-----+--------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+----------+\n|    0|       1|-122.39060867226797|  46.13548501249578|-45.727305937345506|[-122.39060867226...|[1.32875775550098...|[0.93447269679847...|       0.0|\n|    0|       1| 15.798748332829806| -86.21159407546875|   85.2514617870864|[15.7987483328298...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       2|-60.287010425683505| 18.442246406638773|  88.30025324517945|[-60.287010425683...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       3| 122.79284074820067| -88.19527091272191|-185.40334606851977|[122.792840748200...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       3| 137.48274121419482|  -93.9756133320119| -220.9712437858673|[137.482741214194...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       4|-183.01335779637213|  92.07794203262087|  93.60684414506031|[-183.01335779637...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       4|  -57.6008655247749|  5.135393798773895| 236.99158698947267|[-57.600865524774...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       5| -33.87457888688617| 13.353554565156802| -443.7397084372543|[-33.874578886886...|[1.32544819598093...|[0.93406621765391...|       0.0|\n|    0|       5|  144.8763646036816| -43.63767640465293|  36.56313967722145|[144.876364603681...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       6|-56.735799011890066|  81.98728822873187| -17.89864645185649|[-56.735799011890...|[1.40008820121049...|[0.94268535581105...|       0.0|\n|    0|       6|   70.5415741207817| -33.17977745901177|  52.41411079917531|[70.5415741207817...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       7|-4.8281721129789315| -67.38050508399905| 221.24876396496404|[-4.8281721129789...|[1.33821465567488...|[0.93562138094371...|       0.0|\n|    0|       7| 25.023569867449666| -53.21428926366043| -6.843856965266909|[25.0235698674496...|[1.33471022217869...|[0.93519791757187...|       0.0|\n|    0|       8|-137.06896619507458| 149.26988943191122|  -75.2732620551891|[-137.06896619507...|[1.32544819598093...|[0.93406621765391...|       0.0|\n|    0|       8| 222.41256717917048|-128.85606064795286| 113.38920172986577|[222.412567179170...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       9| -75.90996406751051|-17.494944385312504|  428.3077529589892|[-75.909964067510...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|       9|0.15382920098312014| -36.78927546535473|-188.32247039727986|[0.15382920098312...|[1.32570032016416...|[0.93409726575898...|       0.0|\n|    0|      10| -75.90246445601944| 115.50570790739657| 111.11911236055724|[-75.902464456019...|[1.32590267922033...|[0.93412217565278...|       0.0|\n|    0|      10| -13.10170252289096| 107.05426618350924|-384.70104392987804|[-13.101702522890...|[1.32716868386889...|[0.93427781911373...|       0.0|\n|    0|      11| -56.73574214427017|  81.98727523527324| -17.89860240383493|[-56.735742144270...|[1.40008820121049...|[0.94268535581105...|       0.0|\n+-----+--------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "prediction.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9975165562913907"
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"CLASS\")\n# prediction accuracy on training data:    \nbinEval.evaluate(prediction) "
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9984025559105432"
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model = pipeline.fit(df_test)\nprediction = model.transform(df_test)\n# prediction accuracy on test data:\nbinEval.evaluate(prediction)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "If you are happy with the result (I\u2019m happy with > 0.55) share your solution with the others."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This exercise was inspired from the second assignment for the Coursera course: \"Advanced Machine Learning and Signal Processing\"\nReference: https://www.coursera.org/learn/advanced-machine-learning-signal-processing"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}