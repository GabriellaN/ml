{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "REQUIREMENTS: Classify news items into 46 topics.\n\nFor this purpose, create a Multi-layer perceptron (MLP) using Keras.\nSteps to complete the assignment:\n- load the Reuters dataset\n- preprocess the input data\n- build a Sequential Keras model \n- compile the model with a training configuration \n- train your model on the training dataset \n- evaluate your model on the test dataset \n\nINPUT DATASET: We will use the Reuters newswire dataset that consists of 11,228 newswires from Reuters, labeled over 46 topics. \nEach newswire is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\nAs a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\nThis dataset is available through the Keras API.\nREFERENCE: https://keras.io/api/datasets/reuters/"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Keras Model\n\nKeras model represents a neural network model. \nKeras provides two modes to create the model:\n- a simple and easy to use Sequential API \n- a more flexible and advanced Functional API\n\nREFERENCE: \nhttps://www.tutorialspoint.com/keras/keras_models.htm#:~:text=As%20learned%20earlier%2C%20Keras%20model,flexible%20and%20advanced%20Functional%20API.\nhttps://keras.io/guides/sequential_model/"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting tensorflow==2.2.0rc1\n  Downloading tensorflow-2.2.0rc1-cp38-cp38-manylinux2010_x86_64.whl (516.2 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 516.2 MB 7.1 kB/s  eta 0:00:011\ufffd\ufffd\u2588\u258f            | 309.3 MB 99.0 MB/s eta 0:00:03\n\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (3.11.2)\nCollecting tensorflow-estimator<2.3.0,>=2.2.0rc0\n  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 454 kB 79.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.15.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (3.1.0)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.19.2)\nRequirement already satisfied: google-pasta>=0.1.8 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (0.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.1.0)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.35.0)\nRequirement already satisfied: scipy==1.4.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.4.1)\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.1.2)\nCollecting tensorboard<2.2.0,>=2.1.0\n  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8 MB 85.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (0.35.1)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (0.3.3)\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (2.10.0)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (0.10.0)\nRequirement already satisfied: astunparse==1.6.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.6.3)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorflow==2.2.0rc1) (1.12.1)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow==2.2.0rc1) (52.0.0.post20211006)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (3.1.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (1.0.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (2.25.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (0.4.4)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (1.23.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (4.2.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (4.7.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (2.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (1.26.6)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (2021.10.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc1) (3.1.1)\nInstalling collected packages: tensorflow-estimator, tensorboard, tensorflow\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.4.0\n    Uninstalling tensorflow-estimator-2.4.0:\n      Successfully uninstalled tensorflow-estimator-2.4.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.4.1\n    Uninstalling tensorboard-2.4.1:\n      Successfully uninstalled tensorboard-2.4.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.4.3\n    Uninstalling tensorflow-2.4.3:\n      Successfully uninstalled tensorflow-2.4.3\nSuccessfully installed tensorboard-2.1.1 tensorflow-2.2.0rc1 tensorflow-estimator-2.2.0\n"
                }
            ],
            "source": "# TensorFlow is an open-source platform for creating Machine Learning applications. \n# REF: https://www.guru99.com/what-is-tensorflow.html\n\n!pip install tensorflow==2.2.0rc1"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "import tensorflow as tf\nif not tf.__version__ == '2.2.0-rc1':\n    print(tf.__version__)\n    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "IMPORTANT: Restart the kernel by clicking on \"Kernel\"-> \"Restart and Clear Output\" and wait until all output disapears. \nThen your changes are beeing picked up.\n\nWe use Keras Sequential model with only two types of layers: Dense and Dropout. \nIn short, a dropout layer ignores a set of neurons (randomly). This normally is used to prevent the net from overfitting. \nThe Dense layer is a normal fully connected layer in a neuronal network.\n\nSee the picture from https://stackoverflow.com/questions/58830573/in-keras-what-is-a-dense-and-a-dropout-layer\n\nWe also specify a random seed to make our results reproducible. \nAnd we load the Reuters data set:"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n2113536/2110848 [==============================] - 0s 0us/step\n\nREMEMBER that each newswire from x_train is encoded as a sequence of word indexes!\nThe shape of x_train: (8982,) = (number_of_rows, number_of_columns)\nSamples from x_train:\n[list([1, 56, 2, 141, 2, 71, 8, 16, 40, 200, 6, 438, 2, 806, 2, 81, 5, 2, 2, 2, 7, 10, 587, 7, 50, 261, 5, 2, 806, 33, 839, 79, 2, 69, 10, 147, 20, 128, 7, 4, 2, 49, 4, 49, 8, 16, 33, 57, 69, 78, 11, 79, 335, 21, 10, 2, 959, 503, 92, 4, 587, 16, 8, 92, 4, 270, 16, 33, 2, 2, 806, 31, 197, 13, 2, 16, 8, 2, 806, 189, 40, 365, 2, 2, 9, 363, 6, 2, 117, 124, 7, 89, 900, 2, 6, 2, 172, 2, 236, 7, 4, 37, 38, 9, 2, 17, 12])\n list([1, 99, 234, 60, 9, 752, 111, 8, 25, 544, 20, 324, 2, 2, 640, 56, 2, 323, 40, 385, 25, 73, 794, 220, 13, 69, 32, 251, 18, 15, 7, 197, 9, 19, 445, 18, 15, 7, 80, 2, 7, 10, 99, 98, 276, 13, 99, 234, 5, 69, 19, 451, 18, 15, 92, 131, 4, 49, 8, 4, 211, 33, 2, 2, 2, 22, 4, 293, 2, 218, 17, 12])\n list([1, 103, 74, 92, 39, 128, 2, 2, 11, 2, 14, 2, 11, 88, 160, 147, 35, 2, 14, 83, 272, 35, 349, 74, 90, 67, 83, 2, 14, 72, 119, 344, 275, 188, 2, 14, 12, 546, 17, 12])\n ...\n list([1, 486, 341, 147, 26, 14, 147, 26, 255, 219, 252, 68, 146, 91, 102, 17, 12])\n list([1, 56, 2, 697, 149, 8, 16, 2, 4, 2, 73, 2, 106, 33, 2, 400, 5, 4, 2, 340, 2, 225, 139, 480, 4, 2, 5, 25, 584, 2, 300, 7, 982, 2, 145, 52, 2, 4, 181, 2, 2, 6, 276, 179, 518, 6, 2, 2, 8, 7, 10, 241, 2, 40, 349, 4, 2, 2, 2, 21, 4, 2, 327, 5, 2, 2, 410, 4, 2, 2, 5, 2, 73, 2, 2, 2, 9, 2, 282, 5, 942, 19, 11, 82, 2, 19, 352, 2, 663, 340, 9, 119, 2, 663, 2, 9, 691, 23, 905, 17, 12])\n list([1, 424, 2, 9, 2, 415, 265, 2, 2, 8, 36, 41, 30, 2, 6, 593, 4, 131, 148, 107, 16, 299, 45, 57, 195, 2, 458, 36, 118, 10, 232, 882, 2, 2, 144, 41, 30, 10, 536, 5, 131, 795, 24, 114, 30, 2, 107, 490, 2, 10, 131, 148, 6, 444, 567, 4, 386, 290, 36, 8, 36, 41, 2, 6, 2, 4, 131, 140, 2, 7, 54, 586, 131, 2, 474, 729, 144, 57, 85, 2, 7, 706, 6, 744, 4, 140, 28, 4, 89, 320, 2, 97, 837, 476, 6, 302, 225, 29, 490, 344, 272, 35, 15, 7, 995, 43, 359, 5, 271, 448, 386, 436, 51, 16, 299, 45, 439, 293, 131, 848, 21, 66, 833, 2, 8, 36, 299, 45, 680, 4, 181, 66, 474, 52, 29, 41, 2, 242, 194, 37, 38, 182, 13, 619, 2, 6, 761, 92, 2, 2, 7, 10, 66, 474, 23, 2, 36, 8, 52, 2, 2, 242, 2, 518, 5, 395, 2, 66, 474, 6, 2, 678, 21, 2, 203, 43, 34, 344, 931, 52, 29, 17, 12])]\n\nREMEMBER that each newswire is categorised into one of the 46 topics, which will serve as our label stored in y_train.\nThe shape of y_train: (8982,) = (number_of_rows, number_of_columns)\nSamples from y_train:\n[ 4  3  3 ...  3 25 11]\nnum_classes:  46  topics\nTopics have values from:  0  to  45\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
                }
            ],
            "source": "import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import reuters\n\nseed = 1337\nnp.random.seed(seed)\n\nmax_words = 1000\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2, seed=seed)\n\n# Note that we cap the maximum number of words in a news item to 1000 by specifying the *num_words* key word. \n# Also, 20% of the data will be test data and we ensure reproducibility by setting our random seed.\n\nprint()\nprint('REMEMBER that each newswire from x_train is encoded as a sequence of word indexes!') \nprint('The shape of x_train:', x_train.shape, '= (number_of_rows, number_of_columns)')\nprint('Samples from x_train:')\nprint(x_train)\n\nprint()\nprint('REMEMBER that each newswire is categorised into one of the 46 topics, which will serve as our label stored in y_train.')\nprint('The shape of y_train:', y_train.shape, '= (number_of_rows, number_of_columns)')\nprint('Samples from y_train:')\nprint(y_train)\n\nnum_classes = np.max(y_train) + 1  # 46 topics\nprint('num_classes: ', num_classes, ' topics')\nprint('Topics have values from: ', np.min(y_train), ' to ', np.max(y_train))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1. Feature encoding\n\nOur training features are still simple sequences of indexes and we need to further preprocess them, so that we can plug them into a *Dense* layer. For this we use a *Tokenizer* from Keras text preprocessing module. This tokenizer will take an index sequence and map it to a vector of length *max_words=1000*. Each of the 1000 vector positions corresponds to one of the words in our newswire corpus. The output of the tokenizer has a 1 at the i-th position of the vector, if the word corresponding to i is in the description of the newswire, and 0 otherwise. Even if this word appears multiple times, we still just put a 1 into our vector, i.e. our tokenizer is binary. We use this tokenizer to transform both train and test features:"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nThe shape of x_train: (8982, 1000) = (number_of_rows, number_of_columns)\nSamples from x_train:\n[[0. 1. 1. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]\n ...\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]]\n\nThe shape of x_test: (2246, 1000) = (number_of_rows, number_of_columns)\nSamples from x_test:\n[[0. 1. 1. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]\n ...\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]\n [0. 1. 1. ... 0. 0. 0.]]\n"
                }
            ],
            "source": "from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n\nprint()\nprint('The shape of x_train:', x_train.shape, '= (number_of_rows, number_of_columns)')\nprint('Samples from x_train:')\nprint(x_train)\n\nprint()\nprint('The shape of x_test:', x_test.shape, '= (number_of_rows, number_of_columns)')\nprint('Samples from x_test:')\nprint(x_test)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Label encoding\n\nUse to_categorical function to transform both *y_train* and *y_test* into one-hot encoded vectors of length *num_classes*:"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nThe shape of y_train: (8982, 46) = (number_of_rows, number_of_columns)\nSamples from y_train:\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\nThe shape of y_test: (2246, 46) = (number_of_rows, number_of_columns)\nSamples from y_test:\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
                }
            ],
            "source": "# to_categorical(y, num_classes=None, dtype=\"float32\")\n# Converts a vector of integers into a binary matrix representation of the input.\n# Example:\n# >>> a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n# >>> a = tf.constant(a, shape=[4, 4])\n# >>> print(a)\n# tf.Tensor(\n#   [[1. 0. 0. 0.]\n#    [0. 1. 0. 0.]\n#    [0. 0. 1. 0.]\n#    [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)    \n# Arguments:\n#     y: vector to be converted into a matrix (with integers from 0 to num_classes).\n#     num_classes: total number of classes. If None, this would be inferred as the (largest number in y) + 1.\n#     dtype: The data type expected by the input. Default: 'float32'.\n# REFERENCE: https://keras.io/api/utils/python_utils/#to_categorical-function\n\ny_train = ###_YOUR_CODE_GOES_HERE_###\ny_test =  ###_YOUR_CODE_GOES_HERE_###\n\nprint()\nprint('The shape of y_train:', y_train.shape, '= (number_of_rows, number_of_columns)')\nprint('Samples from y_train:')\nprint(y_train)\n\nprint()\nprint('The shape of y_test:', y_test.shape, '= (number_of_rows, number_of_columns)')\nprint('Samples from y_test:')\nprint(y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 3. Model definition\n\nNext, initialise a Keras *Sequential* model and add three layers to it:\n\n    Layer1: a *Dense* layer with input_shape=(max_words,), 512 output units and \"relu\" activation.\n    Layer2: a *Dropout* layer with dropout rate of 50%.\n    Layer3: a *Dense* layer with num_classes output units and \"softmax\" activation.\n    \nREMEMBER:\n\nAn Activation Function is a function that we use to adjust the output of a node. It is also known as the Transfer Function.\nREFERENCE: \nhttps://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\nhttps://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html\n\nActivation functions are crucial for a MLP (Multi Layer Perceptron) in learning and making sense of something complicated.\nTheir main objective is to adjust the output of a node for an easier interpretation. \nThat output now functions as an input for the next layer of the network.\nRemember that in MLP we do the sum of: the products of inputs (X) and their corresponding weights (W), than we add a bias and we apply an activation function, f(x), to get the output of that layer and serve it as an input to the next layer.\nNote: The bias is just a constant number, say 1, which is added for scaling purposes.\nREF: https://www.quora.com/Why-do-neural-networks-need-an-activation-function\nMLP video tutorial (13 minutes): https://youtu.be/MXJQgYgzMMU\n\nReLU or the Rectified Linear Activation Function is a function that will output its input directly if it is positive, otherwise, it will output zero. \nIt has become the default activation function for many types of neural networks because \na model that uses it is easier to train and often achieves better performance.\nREF: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/ \n\nSoftmax Activation Function is a mathematical function that converts a vector of numbers into a vector of probabilities.\nSpecifically, the neural network is configured to output N values, one for each class in the classification task, and the softmax function is used to normalize the outputs, converting them from weighted sum values into probabilities that sum to one. Each value in the output of the softmax function is interpreted as the probability of membership for each class.\nREF: https://machinelearningmastery.com/softmax-activation-function-with-python/"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# Instantiate sequential model:\nmodel = Sequential() \n# Add the first layer: a *Dense* layer with input_shape=(max_words,), 512 output units and \"relu\" activation.\nmodel.add( ###_YOUR_CODE_GOES_HERE_###  \n# Add the second layer: a *Dropout* layer with a dropout rate of 50%.\nmodel.add( ###_YOUR_CODE_GOES_HERE_###\n# Add the third layer: a *Dense* layer with num_classes output units and \"softmax\" activation.\nmodel.add( ###_YOUR_CODE_GOES_HERE_###"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 4. Model compilation\n\nIn the next step, you will compile your Keras model with a training configuration: \n- \"categorical_crossentropy\" as loss function \n- \"adam\" as optimizer \n- \"accuracy\" as evaluation metric\n\nNOTE: In case you get an error regarding h5py, just restart the kernel and start from scratch.\n\nREMEMBER:\nThe loss function is also called the error function or the cost function or the objective function.\n\ncategorical_crossentropy\nCross-entropy is the default loss function to use for multi-class classification problems where the target values are in the set {0, 1, 3, \u2026, n}, where each class is assigned a unique integer value.\nMathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\nCross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\nCross-entropy can be specified as the loss function in Keras by specifying \"categorical_crossentropy\" when compiling the model.\nREF: https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n\nOptimizers are classes or methods used to change the attributes of a machine learning model such as weights and learning rate \nin order to reduce errors or losses. Therefore, optimizers help us get results faster.\nTensorFlow supports 9 optimizer classes, one of which is ADAM.\n\nAdam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters. \nREF: https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "\nmodel.compile( ###_YOUR_CODE_GOES_HERE_###\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 5. Model training and evaluation\n\nNext, define the batch_size for training as 32 and train the model for 5 epochs on *x_train* and *y_train* by using the *fit* method of your model. Then calculate the score for your trained model by running *evaluate* on *x_test* and *y_test* with the same batch size as used in *fit*.\n\nREMEMBER:\n\nEPOCHS\nThe number of epochs defines the number of times that the learning algorithm will work through the entire training dataset.\nAn epoch means training the neural network with all the training data for one cycle. In an epoch, we use all of the data exactly once. \nThe recommendation is to start with a large number of epochs and use Early Stopping to halt training when performance stops improving.\n\nBATCH SIZE\nThe batch size defines the number of samples that will be propagated through the network.\n\nLet's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples from the training dataset and trains the network. Next, it takes the second 100 samples and trains the network again. We can keep doing this procedure until we have propagated all samples through the network. A problem might occur with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is to get the final 50 samples and train the network.\n\nAdvantages of using a batch size < number of all samples:\n- It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory.\n- Typically networks train faster with mini-batches. That's because we update the weights after each propagation. In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) and after each of them we've updated our network's parameters. If we used all samples during propagation we would make only 1 update for the network's parameter.\n\nDisadvantages of using a batch size < number of all samples:\nThe smaller the batch the less accurate the estimate of the gradient will be. In the figure https://i.stack.imgur.com/lU3sx.png, you can see that the direction of the mini-batch gradient (green color) fluctuates much more in comparison to the direction of the full batch gradient (blue color).\nREF: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/5\n281/281 [==============================] - 3s 10ms/step - loss: 1.4045 - accuracy: 0.6836\nEpoch 2/5\n281/281 [==============================] - 2s 8ms/step - loss: 0.7718 - accuracy: 0.8176\nEpoch 3/5\n281/281 [==============================] - 2s 8ms/step - loss: 0.5509 - accuracy: 0.8691\nEpoch 4/5\n281/281 [==============================] - 2s 8ms/step - loss: 0.4358 - accuracy: 0.8937\nEpoch 5/5\n281/281 [==============================] - 2s 8ms/step - loss: 0.3419 - accuracy: 0.9148\n71/71 [==============================] - 0s 2ms/step - loss: 0.8593 - accuracy: 0.8023\n"
                }
            ],
            "source": "batch_size =            ###_YOUR_CODE_GOES_HERE_###\nmodel.fit(              ###_YOUR_CODE_GOES_HERE_###\nscore = model.evaluate( ###_YOUR_CODE_GOES_HERE_###"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "raw",
            "metadata": {},
            "source": "If you have done everything as specified, in particular set the random seed as instructed, your test accuracy should be around 80%: "
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.8023152351379395"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "score[1]"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}