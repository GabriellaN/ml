{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n\n\nREQUIREMENTS \nUse Linear Regression to predict Boston housing prices.\nCompare the results to other ML algorithms.\n\nREFERENCE\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html\nhttps://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/regression.html\n\n\n    "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). \nTherefore, we install Apache Spark in local mode for test purposes only. \nDon't use it in production.\n\nIf running outside Watson Studio, this should work as well. \nIn case you are running in an Apache Spark context outside Watson Studio, remove the Apache Spark setup in the first notebook cells."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 34.5MB/s eta 0:00:01  |\u258e                               | 1.7MB 9.1MB/s eta 0:00:24     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f            | 130.4MB 37.4MB/s eta 0:00:03     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a       | 168.3MB 30.1MB/s eta 0:00:02\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 30.7MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "STEP1. LOAD THE DATA"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>crim</th>\n      <th>zn</th>\n      <th>indus</th>\n      <th>chas</th>\n      <th>nox</th>\n      <th>rm</th>\n      <th>age</th>\n      <th>dis</th>\n      <th>rad</th>\n      <th>tax</th>\n      <th>ptratio</th>\n      <th>black</th>\n      <th>lstat</th>\n      <th>medv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n\n   tax  ptratio   black  lstat  medv  \n0  296     15.3  396.90   4.98  24.0  \n1  242     17.8  396.90   9.14  21.6  \n2  242     17.8  392.83   4.03  34.7  \n3  222     18.7  394.63   2.94  33.4  \n4  222     18.7  396.90   5.33  36.2  "
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "\n# Upload the Boston.csv file from: https://www.kaggle.com/puxama/bostoncsv\n### YOUR CODE HERE ###\n\n# Create pandas dataframe from the uploaded Boston.csv file:\nhouse_pdf = pd.read_csv(body)\nhouse_pdf.head()\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# The input data set is from a Kaggle competition: Housing Values in Suburbs of Boston. \n\n# For each house observation, we have the following information:\n# CRIM \u2014 per capita crime rate by town.\n# ZN \u2014 proportion of residential land zoned for lots over 25,000 sq.ft.\n# INDUS \u2014 proportion of non-retail business acres per town.\n# CHAS \u2014 Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n# NOX \u2014 nitrogen oxides concentration (parts per 10 million).\n# RM \u2014 average number of rooms per dwelling.\n# AGE \u2014 proportion of owner-occupied units built prior to 1940.\n# DIS \u2014 weighted mean of distances to five Boston employment centres.\n# RAD \u2014 index of accessibility to radial highways.\n# TAX \u2014 full-value property-tax rate per $10,000.\n# PTRATIO \u2014 pupil-teacher ratio by town.\n# BLACK \u2014 1000(Bk \u2014 0.63)\u00b2 where Bk is the proportion of blacks by town.\n# LSTAT \u2014 lower status of the population (percent).\n# MEDV \u2014 median value of owner-occupied homes in $1000s. This is the target variable.\n\n# Based on this input data set, we'll try to determine a model that can predict the median value of a given house in the area."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(Unnamed: 0=1, crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.5379999999999999, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, black=396.9, lstat=4.98, medv=24.0)]"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "\n#  Create a Spark SQL context to work with:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsc= SparkContext()\nsqlContext = SQLContext(sc)\n\n# Convert the previous pandas dataframe to a pyspark dataframe with Boston housing information:\nhouse_df = sqlContext.createDataFrame(house_pdf)\nhouse_df.take(1)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "STEP2. EXPLORE THE DATA"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "root\n |-- Unnamed: 0: long (nullable = true)\n |-- crim: double (nullable = true)\n |-- zn: double (nullable = true)\n |-- indus: double (nullable = true)\n |-- chas: long (nullable = true)\n |-- nox: double (nullable = true)\n |-- rm: double (nullable = true)\n |-- age: double (nullable = true)\n |-- dis: double (nullable = true)\n |-- rad: long (nullable = true)\n |-- tax: long (nullable = true)\n |-- ptratio: double (nullable = true)\n |-- black: double (nullable = true)\n |-- lstat: double (nullable = true)\n |-- medv: double (nullable = true)\n\n"
                }
            ],
            "source": "\n# Print the schema in a tree format:\nhouse_df.cache()\nhouse_df.printSchema()\n"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>summary</th>\n      <td>count</td>\n      <td>mean</td>\n      <td>stddev</td>\n      <td>min</td>\n      <td>max</td>\n    </tr>\n    <tr>\n      <th>Unnamed: 0</th>\n      <td>506</td>\n      <td>253.5</td>\n      <td>146.2138844296259</td>\n      <td>1</td>\n      <td>506</td>\n    </tr>\n    <tr>\n      <th>crim</th>\n      <td>506</td>\n      <td>3.6135235573122535</td>\n      <td>8.601545105332491</td>\n      <td>0.00632</td>\n      <td>88.9762</td>\n    </tr>\n    <tr>\n      <th>zn</th>\n      <td>506</td>\n      <td>11.363636363636363</td>\n      <td>23.32245299451514</td>\n      <td>0.0</td>\n      <td>100.0</td>\n    </tr>\n    <tr>\n      <th>indus</th>\n      <td>506</td>\n      <td>11.136778656126504</td>\n      <td>6.860352940897589</td>\n      <td>0.46</td>\n      <td>27.74</td>\n    </tr>\n    <tr>\n      <th>chas</th>\n      <td>506</td>\n      <td>0.0691699604743083</td>\n      <td>0.2539940413404101</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>nox</th>\n      <td>506</td>\n      <td>0.5546950592885372</td>\n      <td>0.11587767566755584</td>\n      <td>0.385</td>\n      <td>0.871</td>\n    </tr>\n    <tr>\n      <th>rm</th>\n      <td>506</td>\n      <td>6.284634387351788</td>\n      <td>0.7026171434153232</td>\n      <td>3.5610000000000004</td>\n      <td>8.78</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>506</td>\n      <td>68.57490118577078</td>\n      <td>28.148861406903595</td>\n      <td>2.9</td>\n      <td>100.0</td>\n    </tr>\n    <tr>\n      <th>dis</th>\n      <td>506</td>\n      <td>3.795042687747034</td>\n      <td>2.10571012662761</td>\n      <td>1.1296</td>\n      <td>12.1265</td>\n    </tr>\n    <tr>\n      <th>rad</th>\n      <td>506</td>\n      <td>9.549407114624506</td>\n      <td>8.707259384239366</td>\n      <td>1</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>tax</th>\n      <td>506</td>\n      <td>408.2371541501976</td>\n      <td>168.53711605495903</td>\n      <td>187</td>\n      <td>711</td>\n    </tr>\n    <tr>\n      <th>ptratio</th>\n      <td>506</td>\n      <td>18.455533596837967</td>\n      <td>2.1649455237144455</td>\n      <td>12.6</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>black</th>\n      <td>506</td>\n      <td>356.67403162055257</td>\n      <td>91.29486438415782</td>\n      <td>0.32</td>\n      <td>396.9</td>\n    </tr>\n    <tr>\n      <th>lstat</th>\n      <td>506</td>\n      <td>12.653063241106723</td>\n      <td>7.141061511348571</td>\n      <td>1.73</td>\n      <td>37.97</td>\n    </tr>\n    <tr>\n      <th>medv</th>\n      <td>506</td>\n      <td>22.532806324110698</td>\n      <td>9.197104087379815</td>\n      <td>5.0</td>\n      <td>50.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                0                   1                    2  \\\nsummary     count                mean               stddev   \nUnnamed: 0    506               253.5    146.2138844296259   \ncrim          506  3.6135235573122535    8.601545105332491   \nzn            506  11.363636363636363    23.32245299451514   \nindus         506  11.136778656126504    6.860352940897589   \nchas          506  0.0691699604743083   0.2539940413404101   \nnox           506  0.5546950592885372  0.11587767566755584   \nrm            506   6.284634387351788   0.7026171434153232   \nage           506   68.57490118577078   28.148861406903595   \ndis           506   3.795042687747034     2.10571012662761   \nrad           506   9.549407114624506    8.707259384239366   \ntax           506   408.2371541501976   168.53711605495903   \nptratio       506  18.455533596837967   2.1649455237144455   \nblack         506  356.67403162055257    91.29486438415782   \nlstat         506  12.653063241106723    7.141061511348571   \nmedv          506  22.532806324110698    9.197104087379815   \n\n                             3        4  \nsummary                    min      max  \nUnnamed: 0                   1      506  \ncrim                   0.00632  88.9762  \nzn                         0.0    100.0  \nindus                     0.46    27.74  \nchas                         0        1  \nnox                      0.385    0.871  \nrm          3.5610000000000004     8.78  \nage                        2.9    100.0  \ndis                     1.1296  12.1265  \nrad                          1       24  \ntax                        187      711  \nptratio                   12.6     22.0  \nblack                     0.32    396.9  \nlstat                     1.73    37.97  \nmedv                       5.0     50.0  "
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "\n# Perform descriptive analytics\nhouse_df.describe().toPandas().transpose()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nPLOT THE DATA USING A SCATTER MATRIX\n\nScatter matrix is a great way to roughly determine if we have a linear correlation between multiple independent variables.\nA scatter matrix (pairs plot) compactly plots all the numeric variables we have in a dataset against each other.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "\n# Sample the data to avoid huge inputs for scatter matrix plot:\nimport pandas as pd\nnumeric_features = [t[0] for t in house_df.dtypes if t[1] == 'int' or t[1] == 'double']\nsampled_data = house_df.select(numeric_features).sample(False, 0.8).toPandas()\n\n# Plot the corresponding scatter matrix:\nfrom pandas.plotting import scatter_matrix\naxs = scatter_matrix(sampled_data, figsize=(10, 10))\nn = len(sampled_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())    \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nThe diagonal of the scatter matrix shows the distribution of all the variables in our test data.\nIn the other cells of the plot matrix, we have the scatterplots (i.e. correlation plot) of each variable combination in our dataframe.\n\nBut it is hard to see if we have a linear correlation between the variables. \nLet\u2019s compute the correlations between the independent variables and the target variable.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Correlation to medv (house median values) for  Unnamed: 0 -0.22660364293533927\nCorrelation to medv (house median values) for  crim -0.38830460858681154\nCorrelation to medv (house median values) for  zn 0.3604453424505433\nCorrelation to medv (house median values) for  indus -0.4837251600283728\nCorrelation to medv (house median values) for  chas 0.1752601771902987\nCorrelation to medv (house median values) for  nox -0.42732077237328203\nCorrelation to medv (house median values) for  rm 0.69535994707154\nCorrelation to medv (house median values) for  age -0.3769545650045961\nCorrelation to medv (house median values) for  dis 0.249928734085904\nCorrelation to medv (house median values) for  rad -0.38162623063977735\nCorrelation to medv (house median values) for  tax -0.46853593356776674\nCorrelation to medv (house median values) for  ptratio -0.5077866855375622\nCorrelation to medv (house median values) for  black 0.3334608196570661\nCorrelation to medv (house median values) for  lstat -0.7376627261740145\nCorrelation to medv (house median values) for  medv 1.0\n"
                }
            ],
            "source": "\nhouse_df.take(1)\n\nimport six\nfor i in house_df.columns: # Take each column of the dataset:\n    if not( isinstance(house_df.select(i).take(1)[0][0], six.string_types)): # If column values are not Strings:\n        print( \"Correlation to medv (house median values) for \", i, house_df.stat.corr('medv', i)) # Check for a corr between the column and the target var (house prices). \n        "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nThe correlation coefficient ranges from \u20131 to 1. \n\nWhen it is close to 1, it means that there is a strong positive correlation; for example, the median value tends to go up when the number of rooms goes up. \nWhen the coefficient is close to \u20131, it means that there is a strong negative correlation; the median value tends to go down when the percentage of the lower status of the population goes up. \nCoefficients close to zero mean that there is no linear correlation.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nWe are going to keep all the variables, for now.\nBut we prepare the data for Machine Learning algorithms, fitting it in only two columns: features and label(\u201cmedv\u201d).\n    "
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------------+----+\n|            features|medv|\n+--------------------+----+\n|[0.00632,18.0,2.3...|24.0|\n|[0.02731,0.0,7.07...|21.6|\n|[0.02729,0.0,7.07...|34.7|\n+--------------------+----+\nonly showing top 3 rows\n\n"
                }
            ],
            "source": "\nfrom pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler(inputCols = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat'], outputCol = 'features')\nvhouse_df = vectorAssembler.transform(house_df)\nvhouse_df = vhouse_df.select(['features', 'medv'])\nvhouse_df.show(3)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nSplit the data in training and test data sets\n"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "splits = vhouse_df.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nSTEP3. LINEAR REGRESSION IMPLEMENTATION WITH SPARK ML\n\nLinear Regression Parameters:\n\nIf your linear model contains many predictor variables X or if these variables are correlated, \nthe parameter estimates have large variance (the model overfits the training data), thus making the model unreliable.\n\nTo counter this, you can use regularization: a technique allowing to decrease this variance at the cost of introducing some bias. \nFinding a good bias-variance trade-off allows to minimize the model's total error.\n\nThere are three popular regularization techniques, each of them aiming at decreasing the size of the coefficients:\n  Lasso Regression, which penalizes the sum of absolute values of the coefficients (L1 penalty).\n  Ridge Regression, which penalizes the sum of squared coefficients (L2 penalty).\n  Elastic Net, a convex combination of Ridge and Lasso meant to take the best from both.\nThe size of the respective penalty terms can be tuned via cross-validation to find the model's best fit.\n\nExample of linear regression parameters choices:\n\nregParam=0.3        This is the penalty strength parameter: the AMOUNT of shrinkage in model parameters number. It is usually noted with lambda \u03bb. \u03bb >= 0.\n\nelasticNetParam=0.8 This is the ratio between L1 and L2 regularization TECHNIQUES. It is usually nothed with alpha (\u03b1). And \u03b1 is in [0, 1] range. \n                    For alpha = 0, the penalty is an L2 or Ridge penalty. \n                    For alpha = 1, the penalty is an L1 or Lasso penalty."
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "\nfrom pyspark.ml.regression import LinearRegression\n\n# Reference: https://spark.apache.org/docs/latest/ml-classification-regression.html\n# Make sure to also specify featuresCol and labelCol\n# lr = ### YOUR CODE HERE ### \n"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Coefficients: [-0.02843385073730485,0.006987958941808668,-0.006018303837825146,2.766194104992275,-1.7764473377226828,4.110793844028774,0.0,-0.5033371587694981,0.0,-0.000521006185267343,-0.7457130584285585,0.010967073089205345,-0.5824436825880731]\nIntercept: 16.888442985401326\n"
                }
            ],
            "source": "\n# Train the model on the training dataset:\nlr_model = lr.fit(train_df)\n\n# Print the coefficients and intercept for linear regression\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"Intercept: \" + str(lr_model.intercept))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Resulting equation: y = cf1 * x1 + cf2 * x2 + ... + cfn * xn + intercept"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nNow, summarize the model over the training set and print out some metrics:\n    "
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Root Mean Squared Error (RMSE) on training data: 4.796013\nR Squared (R2) on training data: 0.723605\n"
                }
            ],
            "source": "\ntrainingSummary = lr_model.summary\nprint(\"Root Mean Squared Error (RMSE) on training data: %f\" % trainingSummary.rootMeanSquaredError) \nprint(\"R Squared (R2) on training data: %f\" % trainingSummary.r2)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nRMSE measures the differences between the values predicted by the model and the actual values. \nHowever, RMSE alone is meaningless until we compare with the actual \u201cmedv\u201d value, such as mean, min and max. \nAfter such comparison, our RMSE looks pretty good.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-------+------------------+\n|summary|              medv|\n+-------+------------------+\n|  count|               362|\n|   mean|22.562430939226516|\n| stddev| 9.135148560725185|\n|    min|               5.0|\n|    max|              50.0|\n+-------+------------------+\n\n"
                }
            ],
            "source": "\ntrain_df.describe().show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nR squared at 0.74 indicates that approximate 74% of the variability in \u201cmedv\u201d can be explained using our model. \nIt is not bad. However, we must be cautious because the performance on the training set may not be a good approximation of the performance on the test data set.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------------------+----+--------------------+\n|        prediction|medv|            features|\n+------------------+----+--------------------+\n|16.933581107514826|18.9|[0.0136,75.0,4.0,...|\n| 41.42348741182059|50.0|[0.01500999999999...|\n| 38.77352246638074|50.0|[0.02009,95.0,2.6...|\n| 36.22357435258677|42.3|[0.02176999999999...|\n| 31.01318747473683|31.1|[0.02187,60.0,2.9...|\n+------------------+----+--------------------+\nonly showing top 5 rows\n\nR Squared (R2) on test data = 0.682175\n"
                }
            ],
            "source": "\n# Make predictions on the test data:\nlr_predictions = lr_model.transform(test_df)\nlr_predictions.select(\"prediction\",\"medv\",\"features\").show(5)\n\n# Evaluate predictions on test data:\nfrom pyspark.ml.evaluation import RegressionEvaluator\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"medv\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Root Mean Squared Error (RMSE) on test data = 5.27127\n"
                }
            ],
            "source": "\ntest_result = lr_model.evaluate(test_df)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nSure enough, we achieved worse RMSE and R squared on the test set.\n\nResiduals\n\nThe difference between the observed value of the dependent variable (y) and the predicted value (\u0177) is called the residual (e). \nEach data point has one residual: e = y - \u0177. Both the sum and the mean of the residuals are equal to zero.\nIdeally all residuals should be small and unstructured (random aspect, no patterns, just vary around 0) \n\nReference: http://www.unige.ch/ses/sococ/cl////stat/action/analyse_residuals0.html?\n"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------------+\n|           residuals|\n+--------------------+\n|  -6.902870284999899|\n|  1.7532368471147741|\n| -5.3865068819629585|\n|  0.7128558346660654|\n|  5.1098134537748585|\n|  12.053323384827635|\n| 0.30757694362829824|\n| -1.3034518517958986|\n| -3.2130995980123096|\n|   9.311248144988383|\n|  2.9490699014573636|\n|  1.8584828656162529|\n| -3.1529447043191325|\n|   6.535236333726118|\n|-0.20552527797233466|\n| -1.5919952668108195|\n| -10.348266220554464|\n|  -3.983768157477524|\n|  -3.936764631984687|\n|  1.7293042154141212|\n+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "trainingSummary.residuals.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "objectiveHistory: [0.5, 0.43245577500547494, 0.2355007629342825, 0.21139271977337173, 0.18068237997375186, 0.17788288174175415, 0.17682238630551858, 0.17589667986395252, 0.17491822484378208, 0.1741280827040414, 0.1740053945759089]\nnumIterations: 11\n"
                }
            ],
            "source": "\n# Print the objective per iteration: the objective per iteration is to minimize the cost function (the error of the model).\n# \n# To define and measure the error of our model we define the cost function as the sum of the squares of the residuals. \n# We should observe that the cost function (the error of the model) decreases with each iteration and converges after a number of iterations.\n# Reference: https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2\n# \nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "LINEAR REGRESSION vs DECISION TREE REGRESSION vs GRADIENT BOOSTING TREE REGRESSION\n\nLINEAR REGRESSION \nIt is used to predict continuous outputs where there is a linear relationship between the features of the dataset and the output variable. \nIt is used for regression problems where you are trying to predict something with infinite possible answers such as the price of a house.\n\nWhen to use linear regression\nIt is appropriate for datasets with a linear relationship between the features and the output variable. \nPOLYNOMIAL REGRESSION can also be used when there is a non-linear relationship between the features and the output.\n\nDECISION TREES\nThey can be used for either classification or regression problems and are useful for complex datasets. \nThey work by splitting the dataset, in a tree-like structure, into smaller and smaller subsets \nand then make predictions based on what subset a new example would fall into.\n\nWhen to use decision trees\nThey are useful when there are complex relationships between the features and the output variables. \nThey also work well compared to other algorithms when there are missing features, when there is a mix of categorical and numerical features \nand when there is a big difference in the scale of features.\n\nDECISION TREE vs RANDOM FOREST vs GRADIENT BOOSTING MACHINES\nA decision tree is a simple, decision making-diagram. \nRandom forests are a large number of trees, combined (using averages or \"majority rules\") at the end of the process. \nGradient boosting machines also combine decision trees, but start the combining process at the beginning, instead of at the end.\n\nREFERENCE: \nhttps://mlcorner.com/linear-regression-vs-decision-trees/\nhttps://www.datasciencecentral.com/profiles/blogs/decision-tree-vs-random-forest-vs-boosted-trees-explained#:~:text=In%20a%20nutshell%3A,instead%20of%20at%20the%20end"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nSTEP4. DECISION TREE REGRESSION WITH SPARK ML\n"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Root Mean Squared Error (RMSE) on test data = 3.75437\nR Squared (R2) on test data = 0.838776\n"
                }
            ],
            "source": "\nfrom pyspark.ml.regression import DecisionTreeRegressor\n\n# Reference: https://spark.apache.org/docs/latest/ml-classification-regression.html\n# Make sure to specify featuresCol and labelCol\n# dt = ### YOUR CODE HERE ###\n\n# Train the model:\ndt_model = dt.fit(train_df)\n\n# Make predictions on the test data:\ndt_predictions = dt_model.transform(test_df)\n\n# Evaluate prediction accuracy:\ndt_evaluator = RegressionEvaluator(\n    labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = dt_evaluator.evaluate(dt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\ndt_evaluator_r2 = RegressionEvaluator(\n    labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"r2\")\nr2 = dt_evaluator_r2.evaluate(dt_predictions)\nprint(\"R Squared (R2) on test data = %g\" % r2)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nFeature Importances \n\nThey are assigned scores to input features showing the relative importance of each feature when making a prediction.\nFeature importances are useful for feature selection: \n    the process of identifying and selecting a subset of input variables that are most relevant to the target variable.\n\nTop reasons to use feature selection are:\n    It enables the machine learning algorithm to train faster.\n    It reduces the complexity of a model and makes it easier to interpret.\n    It improves the accuracy of a model if the right subset is chosen.\n    It reduces overfitting.\n    \nReference: https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/\n        "
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "SparseVector(13, {0: 0.0584, 4: 0.0104, 5: 0.5725, 6: 0.0009, 7: 0.0858, 9: 0.0042, 10: 0.0555, 11: 0.0042, 12: 0.2081})"
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "\n# Feature importances:\ndt_model.featureImportances \n"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[Row(Unnamed: 0=1, crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.5379999999999999, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, black=396.9, lstat=4.98, medv=24.0)]"
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "\n# And the features are:\nhouse_df.take(1)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Apparently, the number of rooms is the most important feature to predict the house median price in our data."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nSTEP5. GRADIENT BOOSTED TREE REGRESSION WITH SPARK ML\n\nREMEMBER:\n\nDECISION TREE vs RANDOM FOREST vs GRADIENT BOOSTING MACHINES\nA decision tree is a simple, decision making-diagram. \nRandom forests are a large number of trees, combined (using averages or \"majority rules\") at the end of the process. \nGradient boosting machines also combine decision trees, but start the combining process at the beginning, instead of at the end.\n\nReference: \nhttps://www.datasciencecentral.com/profiles/blogs/decision-tree-vs-random-forest-vs-boosted-trees-explained#:~:text=In%20a%20nutshell%3A,instead%20of%20at%20the%20end\n"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------------------+----+--------------------+\n|        prediction|medv|            features|\n+------------------+----+--------------------+\n| 15.34769474911794|18.9|[0.0136,75.0,4.0,...|\n| 46.62320773492755|50.0|[0.01500999999999...|\n| 47.10060831831482|50.0|[0.02009,95.0,2.6...|\n|47.221387758214696|42.3|[0.02176999999999...|\n| 30.07703851167657|31.1|[0.02187,60.0,2.9...|\n+------------------+----+--------------------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "\nfrom pyspark.ml.regression import GBTRegressor\n\n# Reference: https://spark.apache.org/docs/latest/ml-classification-regression.html\n# Make sure to also specify featuresCol and labelCol\n# gbt = ### YOUR CODE HERE ###\n\n# Train the model:\ngbt_model = gbt.fit(train_df)\n\n# Make predictions on the test data:\ngbt_predictions = gbt_model.transform(test_df)\ngbt_predictions.select('prediction', 'medv', 'features').show(5)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Root Mean Squared Error (RMSE) on test data = 3.50416\nR Squared (R2) on test data = 0.859549\n"
                }
            ],
            "source": "\n# Evaluate the predictions:\n\ngbt_evaluator = RegressionEvaluator(\n    labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = gbt_evaluator.evaluate(gbt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\ngbt_evaluator_r2 = RegressionEvaluator(\n    labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"r2\")\nr2 = gbt_evaluator_r2.evaluate(gbt_predictions)\nprint(\"R Squared (R2) on test data = %g\" % r2)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "CONCLUSION: \n    Gradient-boosted tree regression performed the best on our data."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "REFERENCE CASE STUDY: https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "OPTIONAL HOMEWORK:\n\nPredict Boston housing prices using a Random Forest algorithm.\n\nREMEMBER:\n\nDECISION TREE vs RANDOM FOREST vs GRADIENT BOOSTING MACHINES \nA decision tree is a simple, decision making-diagram. \nRandom forests are a large number of trees, combined (using averages or \"majority rules\") at the end of the process. \nGradient boosting machines also combine decision trees, but start the combining process at the beginning, instead of at the end.\n\nREFERENCE: \nhttps://spark.apache.org/docs/latest/ml-classification-regression.html\nhttps://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/regression.html"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}