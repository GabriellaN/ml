{
    "cells": [
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install findspark",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Collecting findspark\n  Downloading https://files.pythonhosted.org/packages/fc/2d/2e39f9a023479ea798eed4351cd66f163ce61e00c717e03c37109f00c0f2/findspark-1.4.2-py2.py3-none-any.whl\nInstalling collected packages: findspark\nSuccessfully installed findspark-1.4.2\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import os\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--jars xgboost4j-spark-0.72.jar,xgboost4j-0.72.jar pyspark-shell'\n\nimport findspark\nfindspark.init()\n\nimport pyspark\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "spark = SparkSession\\\n        .builder\\\n        .appName(\"PySpark XGBOOST Titanic\")\\\n        .getOrCreate()",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# spark.sparkContext.addPyFile(\"YOUR_PATH/sparkxgb.zip\")\n# !pip install xgboost==0.72\n# !pip install sparkxgb==0.72\nspark.sparkContext.addPyFile(\"https://github.com/dmlc/xgboost/files/2161553/sparkxgb.zip\")",
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Collecting sparkxgb==0.72\n\u001b[31m  Could not find a version that satisfies the requirement sparkxgb==0.72 (from versions: )\u001b[0m\n\u001b[31mNo matching distribution found for sparkxgb==0.72\u001b[0m\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sparkxgb import XGBoostEstimator",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# schema = StructType(\n#   [StructField(\"PassengerId\", DoubleType()),\n#     StructField(\"Survival\", DoubleType()),\n#     StructField(\"Pclass\", DoubleType()),\n#     StructField(\"Name\", StringType()),\n#     StructField(\"Sex\", StringType()),\n#     StructField(\"Age\", DoubleType()),\n#     StructField(\"SibSp\", DoubleType()),\n#     StructField(\"Parch\", DoubleType()),\n#     StructField(\"Ticket\", StringType()),\n#     StructField(\"Fare\", DoubleType()),\n#     StructField(\"Cabin\", StringType()),\n#     StructField(\"Embarked\", StringType())\n#   ])\n\nschema = StructType(\n  [StructField(\"PassengerId\", IntegerType()),\n    StructField(\"Survival\", IntegerType()),\n    StructField(\"Pclass\", IntegerType()),\n    StructField(\"Name\", StringType()),\n    StructField(\"Sex\", StringType()),\n    StructField(\"Age\", DoubleType()),\n    StructField(\"SibSp\", IntegerType()),\n    StructField(\"Parch\", IntegerType()),\n    StructField(\"Ticket\", StringType()),\n    StructField(\"Fare\", DoubleType()),\n    StructField(\"Cabin\", StringType()),\n    StructField(\"Embarked\", StringType())\n  ])",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# df_raw = spark\\\n#   .read\\\n#   .option(\"header\", \"true\")\\\n#   .schema(schema)\\\n#   .csv(\"YOUR_PATH/train.csv\")\n\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_d71b671c91294df198c5106d89a19b39 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='obwQpsgifzkvBXAUeiUcXF1EVD0-ePgPBdKzu8fnFZOm',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_d71b671c91294df198c5106d89a19b39.get_object(Bucket='default-donotdelete-pr-uwersfb6mbkrmv',Key='train.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ntrain_pd = pd.read_csv(body)\n# train_pd.head()\ndf_raw = spark.createDataFrame(train_pd, schema)\ndf_raw.head()\n",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 10,
                    "data": {
                        "text/plain": "Row(PassengerId=1, Survival=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin='NaN', Embarked='S')"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_raw.show(2)",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n|PassengerId|Survival|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171|   7.25|  NaN|       S|\n|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.2833|  C85|       C|\n+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\nonly showing top 2 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df = df_raw.na.fill(0)",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sexIndexer = StringIndexer()\\\n  .setInputCol(\"Sex\")\\\n  .setOutputCol(\"SexIndex\")\\\n  .setHandleInvalid(\"keep\")\n    \ncabinIndexer = StringIndexer()\\\n  .setInputCol(\"Cabin\")\\\n  .setOutputCol(\"CabinIndex\")\\\n  .setHandleInvalid(\"keep\")\n    \nembarkedIndexer = StringIndexer()\\\n  .setInputCol(\"Embarked\")\\\n  .setOutputCol(\"EmbarkedIndex\")\\\n  .setHandleInvalid(\"keep\")",
            "execution_count": 13,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "vectorAssembler = VectorAssembler()\\\n  .setInputCols([\"Pclass\", \"SexIndex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"CabinIndex\", \"EmbarkedIndex\"])\\\n  .setOutputCol(\"features\")",
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "xgboost = XGBoostEstimator(\n    featuresCol=\"features\", \n    labelCol=\"Survival\", \n    predictionCol=\"prediction\"\n)",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "TypeError",
                    "evalue": "'JavaPackage' object is not callable",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-15-b3f19f8c0cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Survival\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/tmp/spark/scratch/spark-7bf57bd5-bdcf-4821-b0d7-a2eae0b52c44/userFiles-fdd21788-2f22-44b3-b1aa-f7fead3ea8ee/sparkxgb.zip/sparkxgb/xgboost.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, checkpoint_path, checkpointInterval, missing, nthread, nworkers, silent, use_external_memory, baseMarginCol, featuresCol, labelCol, predictionCol, weightCol, base_score, booster, eval_metric, num_class, num_round, objective, seed, alpha, colsample_bytree, colsample_bylevel, eta, gamma, grow_policy, max_bin, max_delta_step, max_depth, min_child_weight, reg_lambda, scale_pos_weight, sketch_eps, subsample, tree_method, normalize_type, rate_drop, sample_type, skip_drop, lambda_bias)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXGBoostEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ml.dmlc.xgboost4j.scala.spark.XGBoostEstimator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_params_from_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         self._setDefault(\n",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
                    ]
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pipeline = Pipeline().setStages([sexIndexer, cabinIndexer, embarkedIndexer, vectorAssembler, xgboost])",
            "execution_count": 20,
            "outputs": []
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "trainDF, testDF = df.randomSplit([0.8, 0.2], seed=24)",
            "execution_count": 24,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = pipeline.fit(trainDF)",
            "execution_count": 26,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model.transform(testDF).select(col(\"PassengerId\"), col(\"prediction\")).show()",
            "execution_count": 32,
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-----------+----------+\n|PassengerId|prediction|\n+-----------+----------+\n|        1.0|       0.0|\n|        4.0|       1.0|\n|       14.0|       0.0|\n|       15.0|       1.0|\n|       20.0|       1.0|\n|       28.0|       1.0|\n|       34.0|       0.0|\n|       38.0|       0.0|\n|       50.0|       1.0|\n|       52.0|       0.0|\n|       59.0|       1.0|\n|       60.0|       0.0|\n|       82.0|       0.0|\n|       94.0|       0.0|\n|       96.0|       0.0|\n|       99.0|       1.0|\n|      104.0|       0.0|\n|      105.0|       0.0|\n|      107.0|       1.0|\n|      116.0|       0.0|\n+-----------+----------+\nonly showing top 20 rows\n\n"
                }
            ]
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}